#### Abstract

_We present Okutama-Action, a new video dataset for aerial view concurrent human action detection. It consists of 43 minute-long fully-annotated sequences with 12 action classes. Okutama-Action features many challenges missing in current datasets, including dynamic transition of actions, significant changes in scale and aspect ratio, abrupt camera movement, as well as multi-labeled actors. As a result, our dataset is more challenging than existing ones, and will help push the field forward to enable real-world applications._

#### Publications

[Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action Detection](https://arxiv.org/abs/1706.03038)


#### Download

- Sample (one 4K video and labels): [sample](https://okutama-action.s3.eu-central-1.amazonaws.com/Sample.zip)

We offer the dataset in two different formats:

- 1280x720 frames + labels: [train set](https://okutama-action.s3.eu-central-1.amazonaws.com/TrainSetFrames.zip), [test set](https://okutama-action.s3.eu-central-1.amazonaws.com/TestSetFrames.zip)
- 4K videos + labels: [train set](https://okutama-action.s3.eu-central-1.amazonaws.com/TrainSetVideos.zip), [test set](https://okutama-action.s3.eu-central-1.amazonaws.com/TestSetVideos.zip)

In addition, we provide trained models in Caffe: [models](https://okutama-action.s3.eu-central-1.amazonaws.com/FinalModels.zip)

#### About

The creation of this dataset was supported by [Prendinger Lab](http://research.nii.ac.jp/~prendinger/) at the [National Institute of Informatics](http://www.nii.ac.jp/en/), Tokyo, Japan.
